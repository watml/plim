{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"17_mnist.pkl\", \"br\") as fh:\n",
    "    data = pickle.load(fh)\n",
    "\n",
    "train_imgs = data[0]\n",
    "test_imgs = data[1]\n",
    "train_labels = data[2]\n",
    "test_labels = data[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i]==7:\n",
    "        train_labels[i]=np.array([0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.FloatTensor(train_imgs)\n",
    "train_y = torch.FloatTensor(train_labels)\n",
    "#train_y = train_y.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13007, 1])\n"
     ]
    }
   ],
   "source": [
    "print(train_y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        assert X.size()[0] == y.size()[0]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.size()[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "device='cpu'\n",
    "\n",
    "class LR(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=128, num_class=1):\n",
    "        super(LR, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, num_class)\n",
    "        self.act=nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(LinearDataset(train_x, train_y), batch_size=len(train_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LR().to(device).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "state_dict['fc1.weight'] = torch.ones([1,784])\n",
    "state_dict['fc1.bias'] = torch.Tensor([-0.001])\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        #print(name, param.data)\n",
    "        break\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> start gradient canceling attack with given target parameters\n",
      "==> model will be saved in poisoned_models\n",
      "tensor(626500., dtype=torch.float64, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "the necessary size of epsilon_d:(28.765904788638146+0j)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adadelta(model.parameters(), lr=0.1)\n",
    "# define epsilon for one point for now\n",
    "epsilon = 1/len(train_imgs)\n",
    "# alpha may indicate the scaling factor?\n",
    "alpha = 0\n",
    "\n",
    "\n",
    "def autograd(outputs, inputs, create_graph=False):\n",
    "    \"\"\"Compute gradient of outputs w.r.t. inputs, assuming outputs is a scalar.\"\"\"\n",
    "    #inputs = tuple(inputs)\n",
    "    grads = torch.autograd.grad(outputs, inputs, create_graph=create_graph, allow_unused=True)\n",
    "    return [xx if xx is not None else yy.new_zeros(yy.size()) for xx, yy in zip(grads, inputs)]\n",
    "\n",
    "from scipy.special import lambertw\n",
    "import math\n",
    "lambert_w = lambertw(1/math.e)\n",
    "\n",
    "# load gmu to initialize the poisoned samples\n",
    "g1 = torch.load('gmu.pt')\n",
    "\n",
    "loss_all = []\n",
    "def attack(epoch,lr):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device).double(), target.to(device).double()\n",
    "        data.requires_grad=True\n",
    "        if epoch==0:\n",
    "            # initialize poisoned data with 1/epsilon g(mu)\n",
    "            #data_p = Variable(data[:(int(epsilon*len(data)))])\n",
    "            data_p = Variable(g1.repeat(epsilon))\n",
    "            target_p = Variable(target[:(int(epsilon*len(target)))])\n",
    "            torch.save(target_p,'poisoned_models/lr/target_p_{}.pt'.format(epsilon))\n",
    "        else:\n",
    "            data_p = torch.load('poisoned_models/lr/data_p_{}.pt'.format(epsilon))\n",
    "            target_p = torch.load('poisoned_models/lr/target_p_{}.pt'.format(epsilon))\n",
    "        data_p.requires_grad=True\n",
    "        # initialize f function\n",
    "        criterion = nn.BCELoss(reduction='sum')\n",
    "        \n",
    "        # calculate gradient of w on clean sample\n",
    "        output_c = model(data.view(data.size(0), -1))\n",
    "        #output_c = model(data)\n",
    "        loss_c = criterion(output_c,target)\n",
    "        # wrt to w here\n",
    "        grad_c= autograd(loss_c,tuple(model.parameters()),create_graph=True)\n",
    "        g1 = grad_c[0]\n",
    "        \n",
    "        # calculate gradient of w on poisoned sample\n",
    "        output_p = model(data_p.view(data_p.size(0), -1))\n",
    "        loss_p = criterion(output_p,target_p)\n",
    "        grad_p= autograd(loss_p,tuple(model.parameters()),create_graph=True)\n",
    "        g2 = grad_p[0]\n",
    "        \n",
    "        # calculate the true loss: |g_c + g_p|_{inf}\n",
    "        \n",
    "        grad_sum = g1+g2\n",
    "\n",
    "        \n",
    "        loss = torch.norm(grad_sum,2)\n",
    "        loss_all.append(loss.detach().cpu().numpy())\n",
    "        if loss < 1:\n",
    "            break\n",
    "            \n",
    "        update = autograd(loss,data_p,create_graph=True)\n",
    "        \n",
    "        data_t = data_p - lr * update[0]\n",
    "\n",
    "        torch.save(data_t, 'poisoned_models/lr/data_p_{}.pt'.format(epsilon))\n",
    "        \n",
    "        \n",
    "        print(\"epoch:{},loss:{},lr:{}\".format(epoch, loss,lr))\n",
    "\n",
    "        \n",
    "        \n",
    "print(\"==> start gradient canceling attack with given target parameters\")\n",
    "print(\"==> model will be saved in poisoned_models\")\n",
    "epochs=20\n",
    "lr=1\n",
    "for epoch in range(epochs):\n",
    "    attack(epoch,lr)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(a.repeat(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
