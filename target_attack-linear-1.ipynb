{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target parameter attack on linear regression with close form solution on the cross derivative\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from models import *\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 1\n",
    "num_outputs = 1\n",
    "num_examples_train = 10000\n",
    "num_examples_test = 5000\n",
    "dtype = torch.float\n",
    "\n",
    "def real_fn(X):\n",
    "    return 2 * X + 4.2\n",
    "    #return 2 * X[:, 0] - 3.4 * X[:, 1] + 4.2\n",
    "\n",
    "# define training set\n",
    "X_train = torch.randn(num_examples_train, num_inputs, device=device, dtype=dtype)\n",
    "noise_train = .1 * torch.randn(num_examples_train, num_inputs, device=device, dtype=dtype)\n",
    "y_train = (real_fn(X_train) + noise_train)\n",
    "\n",
    "X_test = torch.randn(num_examples_test, num_inputs, device=device, dtype=dtype)\n",
    "noise_test = .1 * torch.randn(num_examples_test, num_inputs, device=device, dtype=dtype)\n",
    "y_test = (real_fn(X_test) + noise_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        assert X.size()[0] == y.size()[0]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.size()[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "train_data = DataLoader(LinearDataset(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "test_data = DataLoader(LinearDataset(X_test, y_test), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LinearRegression, self).__init__(**kwargs)\n",
    "        self.fc1 = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "model = LinearRegression()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"models/linear_gd.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the script for training target attack\n",
    "epsilon = 1e-4\n",
    "lr = 0.05\n",
    "epochs = 200\n",
    "decay_rate = 0.96\n",
    "decay_steps = 10000\n",
    "\n",
    "def adjust_learning_rate(lr, epoch):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    lr *= 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n",
    "    return(lr)\n",
    "\n",
    "\n",
    "def autograd(outputs, inputs, create_graph=False):\n",
    "    \"\"\"Compute gradient of outputs w.r.t. inputs, assuming outputs is a scalar.\"\"\"\n",
    "    #inputs = tuple(inputs)\n",
    "    grads = torch.autograd.grad(outputs, inputs, create_graph=create_graph, allow_unused=True)\n",
    "    return [xx if xx is not None else yy.new_zeros(yy.size()) for xx, yy in zip(grads, inputs)]\n",
    "\n",
    "def train(epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_data):\n",
    "        lr1 = adjust_learning_rate(lr,epoch)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data.requires_grad=True\n",
    "        if epoch==0:\n",
    "            # initialize poisoned data\n",
    "            data_p = Variable(data[:(int(epsilon*len(data)))])\n",
    "            target_p = Variable(target[:(int(epsilon*len(target)))])\n",
    "            max_value = torch.max(data_p)\n",
    "            min_value = torch.min(data_p)\n",
    "            torch.save(target_p,'target_p_linear.pt')\n",
    "        else:\n",
    "            data_p = torch.load('data_p_linear.pt')\n",
    "            target_p = torch.load('target_p_linear.pt')\n",
    "            max_value = torch.max(data_p)\n",
    "            min_value = torch.min(data_p)\n",
    "        data_p.requires_grad=True\n",
    "        \n",
    "    \n",
    "        # initialize f function\n",
    "        criterion = nn.MSELoss(reduction='sum')\n",
    "        \n",
    "        # calculate gradient of w on clean sample\n",
    "        output_c = model(data.view(data.size(0), -1))\n",
    "        loss_c =  0.5 * criterion(output_c,target)\n",
    "        \n",
    "        # calculate dL/dg_1\n",
    "        grad_c = autograd(loss_c,tuple(model.parameters()),create_graph=True)\n",
    "        #g1 = torch.cat((grad_c[0],grad_c[1].unsqueeze(0)),0)\n",
    "        g1 = grad_c[0]\n",
    "        \n",
    "        # calculate gradient of w on poisoned sample\n",
    "        output_p = model(data_p.view(data_p.size(0), -1))\n",
    "        loss_p = 0.5 * criterion(output_p,target_p)\n",
    "        grad_p= autograd(loss_p,tuple(model.parameters()),create_graph=True)\n",
    "        #g2 = torch.cat((grad_p[0],grad_p[1].unsqueeze(0)),0)\n",
    "        #g2 = torch.matmul((output_p - target_p).t(),data_p)\n",
    "        g2 = grad_p[0]\n",
    "        \n",
    "        # calculate the true loss: |g_c + g_p|_{inf}\n",
    "        grad_sum = g1+g2\n",
    "        \n",
    "        loss = torch.norm(grad_sum,2)\n",
    "        if loss < 0.1:\n",
    "            break\n",
    "        \n",
    "        update = autograd(loss,data_p,create_graph=True)\n",
    "    \n",
    "        data_t = data_p - lr1 *update[0]\n",
    "        torch.save(data_t, 'data_p_linear.pt')\n",
    "        \n",
    "        print(\"epoch:{},lr:{},loss:{}\".format(epoch, lr1,loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,lr:0.05,loss:4500.52587890625\n",
      "epoch:1,lr:0.04999691581204152,loss:4500.52490234375\n",
      "epoch:2,lr:0.04998766400914329,loss:4500.5234375\n",
      "epoch:3,lr:0.049972246874049255,loss:4500.52197265625\n",
      "epoch:4,lr:0.04995066821070679,loss:4500.51953125\n",
      "epoch:5,lr:0.0499229333433282,loss:4500.51611328125\n",
      "epoch:6,lr:0.049889049115077,loss:4500.51220703125\n",
      "epoch:7,lr:0.0498490238863795,loss:4500.50634765625\n",
      "epoch:8,lr:0.04980286753286195,loss:4500.4990234375\n",
      "epoch:9,lr:0.04975059144291394,loss:4500.48876953125\n",
      "epoch:10,lr:0.04969220851487845,loss:4500.47509765625\n",
      "epoch:11,lr:0.04962773315386935,loss:4500.45751953125\n",
      "epoch:12,lr:0.049557181268217225,loss:4500.43359375\n",
      "epoch:13,lr:0.049480570265544144,loss:4500.40185546875\n",
      "epoch:14,lr:0.049397919048468686,loss:4500.359375\n",
      "epoch:15,lr:0.049309248009941915,loss:4500.30322265625\n",
      "epoch:16,lr:0.04921457902821578,loss:4500.22900390625\n",
      "epoch:17,lr:0.049113935461444956,loss:4500.1298828125\n",
      "epoch:18,lr:0.04900734214192358,loss:4499.998046875\n",
      "epoch:19,lr:0.048894825369958254,loss:4499.82373046875\n",
      "epoch:20,lr:0.048776412907378844,loss:4499.59228515625\n",
      "epoch:21,lr:0.048652133970688634,loss:4499.2861328125\n",
      "epoch:22,lr:0.04852201922385564,loss:4498.880859375\n",
      "epoch:23,lr:0.04838610077074669,loss:4498.34521484375\n",
      "epoch:24,lr:0.04824441214720629,loss:4497.63720703125\n",
      "epoch:25,lr:0.04809698831278217,loss:4496.70263671875\n",
      "epoch:26,lr:0.04794386564209953,loss:4495.4697265625\n",
      "epoch:27,lr:0.04778508191588613,loss:4493.845703125\n",
      "epoch:28,lr:0.04762067631165049,loss:4491.7080078125\n",
      "epoch:29,lr:0.047450689394015394,loss:4488.896484375\n",
      "epoch:30,lr:0.047275163104709195,loss:4485.20361328125\n",
      "epoch:31,lr:0.047094140752217344,loss:4480.35693359375\n",
      "epoch:32,lr:0.04690766700109659,loss:4474.00439453125\n",
      "epoch:33,lr:0.04671578786095479,loss:4465.68603515625\n",
      "epoch:34,lr:0.046518550675098594,loss:4454.806640625\n",
      "epoch:35,lr:0.04631600410885231,loss:4440.5947265625\n",
      "epoch:36,lr:0.04610819813755038,loss:4422.0517578125\n",
      "epoch:37,lr:0.04589518403420676,loss:4397.88818359375\n",
      "epoch:38,lr:0.04567701435686405,loss:4366.4404296875\n",
      "epoch:39,lr:0.04545374293562559,loss:4325.5654296875\n",
      "epoch:40,lr:0.04522542485937369,loss:4272.5087890625\n",
      "epoch:41,lr:0.04499211646217727,loss:4203.73291015625\n",
      "epoch:42,lr:0.04475387530939226,loss:4114.70458984375\n",
      "epoch:43,lr:0.044510760183458245,loss:3999.62353515625\n",
      "epoch:44,lr:0.044262831069394735,loss:3851.08154296875\n",
      "epoch:45,lr:0.04401014914000078,loss:3659.63134765625\n",
      "epoch:46,lr:0.043752776740761494,loss:3413.25\n",
      "epoch:47,lr:0.043490777374465245,loss:3096.6611328125\n",
      "epoch:48,lr:0.04322421568553529,loss:2690.4921875\n",
      "epoch:49,lr:0.04295315744407972,loss:2170.22216796875\n",
      "epoch:50,lr:0.04267766952966369,loss:1504.87158203125\n",
      "epoch:51,lr:0.04239781991480786,loss:655.376708984375\n",
      "epoch:52,lr:0.04211367764821722,loss:427.4208984375\n",
      "epoch:53,lr:0.041825312837744336,loss:780.785400390625\n",
      "epoch:54,lr:0.0415327966330913,loss:251.68896484375\n",
      "epoch:55,lr:0.041236201208254594,loss:898.474609375\n",
      "epoch:56,lr:0.040935599743717244,loss:86.4013671875\n",
      "epoch:57,lr:0.04063106640839264,loss:1008.883544921875\n",
      "epoch:58,lr:0.040322676341324415,loss:69.0390625\n",
      "epoch:59,lr:0.040010505633147106,loss:1114.19482421875\n",
      "epoch:60,lr:0.03969463130731184,loss:198.228515625\n",
      "epoch:61,lr:0.03937513130108197,loss:931.57861328125\n",
      "epoch:62,lr:0.03905208444630327,loss:319.31396484375\n",
      "epoch:63,lr:0.0387255704499533,loss:759.90234375\n",
      "epoch:64,lr:0.038395669874474916,loss:432.757568359375\n",
      "epoch:65,lr:0.03806246411789872,loss:598.53515625\n",
      "epoch:66,lr:0.03772603539375929,loss:538.992919921875\n",
      "epoch:67,lr:0.03738646671081019,loss:446.87939453125\n",
      "epoch:68,lr:0.037043841852542884,loss:638.427978515625\n",
      "epoch:69,lr:0.036698245356514336,loss:304.37109375\n",
      "epoch:70,lr:0.03634976249348867,loss:731.44873046875\n",
      "epoch:71,lr:0.035998479246397874,loss:170.47998046875\n",
      "epoch:72,lr:0.03564448228912682,loss:818.417236328125\n",
      "epoch:73,lr:0.035287858965127726,loss:44.7060546875\n",
      "epoch:74,lr:0.034928697265869516,loss:899.6748046875\n",
      "epoch:75,lr:0.03456708580912725,loss:73.42236328125\n",
      "epoch:76,lr:0.03420311381711696,loss:931.36767578125\n",
      "epoch:77,lr:0.033836871094481434,loss:164.50048828125\n",
      "epoch:78,lr:0.033468448006132294,loss:797.7900390625\n",
      "epoch:79,lr:0.033097935454953736,loss:249.43310546875\n",
      "epoch:80,lr:0.032725424859373686,loss:672.3955078125\n",
      "epoch:81,lr:0.0323510081308076,loss:328.56494140625\n",
      "epoch:82,lr:0.03197477765098074,loss:554.71044921875\n",
      "epoch:83,lr:0.03159682624913432,loss:402.21826171875\n",
      "epoch:84,lr:0.031217247179121367,loss:444.28759765625\n",
      "epoch:85,lr:0.03083613409639764,loss:470.697509765625\n",
      "epoch:86,lr:0.03045358103491357,loss:340.7080078125\n",
      "epoch:87,lr:0.03006968238391282,loss:534.2890625\n",
      "epoch:88,lr:0.02968453286464312,loss:243.5751953125\n",
      "epoch:89,lr:0.029298227506985238,loss:593.263916015625\n",
      "epoch:90,lr:0.028910861626005774,loss:152.517578125\n",
      "epoch:91,lr:0.028522530798439572,loss:647.878173828125\n",
      "epoch:92,lr:0.028133330839107615,loss:67.1826171875\n",
      "epoch:93,lr:0.027743357777276136,loss:698.37255859375\n",
      "epoch:94,lr:0.02735270783296286,loss:12.7587890625\n",
      "epoch:95,lr:0.026961477393196126,loss:784.62548828125\n",
      "epoch:96,lr:0.02656976298823284,loss:65.8974609375\n",
      "epoch:97,lr:0.026177661267741067,loss:698.595703125\n",
      "epoch:98,lr:0.02578526897695321,loss:114.79443359375\n",
      "epoch:99,lr:0.02539268293279552,loss:618.08447265625\n",
      "epoch:100,lr:0.025,loss:159.6865234375\n",
      "epoch:101,lr:0.02460731706720449,loss:542.77392578125\n",
      "epoch:102,lr:0.024214731023046793,loss:200.79833984375\n",
      "epoch:103,lr:0.023822338732258936,loss:472.359375\n",
      "epoch:104,lr:0.023430237011767167,loss:238.3447265625\n",
      "epoch:105,lr:0.02303852260680388,loss:406.55615234375\n",
      "epoch:106,lr:0.022647292167037144,loss:272.52880859375\n",
      "epoch:107,lr:0.02225664222272387,loss:345.095703125\n",
      "epoch:108,lr:0.0218666691608924,loss:303.54443359375\n",
      "epoch:109,lr:0.02147746920156044,loss:287.7216796875\n",
      "epoch:110,lr:0.021089138373994232,loss:331.57666015625\n",
      "epoch:111,lr:0.02070177249301476,loss:234.19287109375\n",
      "epoch:112,lr:0.020315467135356886,loss:356.80322265625\n",
      "epoch:113,lr:0.01993031761608719,loss:184.28125\n",
      "epoch:114,lr:0.019546418965086444,loss:379.392578125\n",
      "epoch:115,lr:0.019163865903602362,loss:137.76953125\n",
      "epoch:116,lr:0.01878275282087863,loss:399.50732421875\n",
      "epoch:117,lr:0.01840317375086568,loss:94.45361328125\n",
      "epoch:118,lr:0.018025222349019272,loss:417.302490234375\n",
      "epoch:119,lr:0.017648991869192405,loss:54.13720703125\n",
      "epoch:120,lr:0.017274575140626323,loss:432.92822265625\n",
      "epoch:121,lr:0.016902064545046263,loss:16.63818359375\n",
      "epoch:122,lr:0.016531551993867716,loss:446.52734375\n",
      "epoch:123,lr:0.016163128905518576,loss:18.21923828125\n",
      "epoch:124,lr:0.01579688618288306,loss:444.52685546875\n",
      "epoch:125,lr:0.015432914190872763,loss:30.19580078125\n",
      "epoch:126,lr:0.015071302734130482,loss:409.974609375\n",
      "epoch:127,lr:0.014712141034872282,loss:40.29931640625\n",
      "epoch:128,lr:0.014355517710873185,loss:377.90625\n",
      "epoch:129,lr:0.014001520753602122,loss:48.6689453125\n",
      "epoch:130,lr:0.013650237506511332,loss:348.16015625\n",
      "epoch:131,lr:0.01330175464348567,loss:55.4365234375\n",
      "epoch:132,lr:0.012956158147457115,loss:320.58544921875\n",
      "epoch:133,lr:0.01261353328918981,loss:60.732421875\n",
      "epoch:134,lr:0.012273964606240717,loss:295.03564453125\n",
      "epoch:135,lr:0.01193753588210128,loss:64.6787109375\n",
      "epoch:136,lr:0.01160433012552508,loss:271.37548828125\n",
      "epoch:137,lr:0.011274429550046703,loss:67.396484375\n",
      "epoch:138,lr:0.010947915553696733,loss:249.47509765625\n",
      "epoch:139,lr:0.010624868698918037,loss:68.998046875\n",
      "epoch:140,lr:0.010305368692688175,loss:229.2119140625\n",
      "epoch:141,lr:0.009989494366852904,loss:69.5947265625\n",
      "epoch:142,lr:0.009677323658675586,loss:210.47216796875\n",
      "epoch:143,lr:0.00936893359160737,loss:69.29052734375\n",
      "epoch:144,lr:0.009064400256282757,loss:193.146484375\n",
      "epoch:145,lr:0.008763798791745412,loss:68.1865234375\n",
      "epoch:146,lr:0.008467203366908708,loss:177.1328125\n",
      "epoch:147,lr:0.008174687162255665,loss:66.37939453125\n",
      "epoch:148,lr:0.007886322351782783,loss:162.3349609375\n",
      "epoch:149,lr:0.0076021800851921425,loss:63.9609375\n",
      "epoch:150,lr:0.0073223304703363135,loss:148.66455078125\n",
      "epoch:151,lr:0.007046842555920283,loss:61.0185546875\n",
      "epoch:152,lr:0.006775784314464717,loss:136.037109375\n",
      "epoch:153,lr:0.006509222625534755,loss:57.6337890625\n",
      "epoch:154,lr:0.0062472232592385105,loss:124.375\n",
      "epoch:155,lr:0.005989850859999227,loss:53.8876953125\n",
      "epoch:156,lr:0.005737168930605272,loss:113.6064453125\n",
      "epoch:157,lr:0.005489239816541761,loss:49.85205078125\n",
      "epoch:158,lr:0.00524612469060774,loss:103.6630859375\n",
      "epoch:159,lr:0.005007883537822735,loss:45.59765625\n",
      "epoch:160,lr:0.004774575140626317,loss:94.48583984375\n",
      "epoch:161,lr:0.004546257064374418,loss:41.1884765625\n",
      "epoch:162,lr:0.004322985643135957,loss:86.01708984375\n",
      "epoch:163,lr:0.004104815965793249,loss:36.68505859375\n",
      "epoch:164,lr:0.003891801862449629,loss:78.20654296875\n",
      "epoch:165,lr:0.0036839958911477014,loss:32.14404296875\n",
      "epoch:166,lr:0.0034814493249014063,loss:71.00634765625\n",
      "epoch:167,lr:0.0032842121390452175,loss:27.6162109375\n",
      "epoch:168,lr:0.0030923329989034107,loss:64.376953125\n",
      "epoch:169,lr:0.0029058592477826635,loss:23.14892578125\n",
      "epoch:170,lr:0.0027248368952908055,loss:58.27978515625\n",
      "epoch:171,lr:0.002549310605984612,loss:18.7861328125\n",
      "epoch:172,lr:0.0023793236883495163,loss:52.68115234375\n",
      "epoch:173,lr:0.002214918084113873,loss:14.56640625\n",
      "epoch:174,lr:0.0020561343579004773,loss:47.552734375\n",
      "epoch:175,lr:0.0019030116872178371,loss:10.5244140625\n",
      "epoch:176,lr:0.0017555878527937164,loss:42.86962890625\n",
      "epoch:177,lr:0.0016138992292533156,loss:6.6904296875\n",
      "epoch:178,lr:0.0014779807761443637,loss:38.6103515625\n",
      "epoch:179,lr:0.0013478660293113677,loss:3.09228515625\n",
      "epoch:180,lr:0.0012235870926211618,loss:34.75537109375\n",
      "epoch:181,lr:0.001105174630041747,loss:0.2470703125\n",
      "epoch:182,lr:0.0009926578580764262,loss:30.69091796875\n",
      "epoch:183,lr:0.0008860645385550509,loss:3.00390625\n",
      "epoch:184,lr:0.0007854209717842259,loss:21.85888671875\n",
      "epoch:185,lr:0.0006907519900580862,loss:0.244140625\n",
      "epoch:186,lr:0.0006020809515313169,loss:19.14453125\n",
      "epoch:187,lr:0.0005194297344558535,loss:2.20654296875\n",
      "epoch:188,lr:0.000442818731782782,loss:12.35400390625\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.ones(int(num_examples_train*epsilon))\n",
    "b = b.unsqueeze(1)\n",
    "b = b.to('cuda')\n",
    "# absorb b into the input so it matches the weights in tensors\n",
    "X_p_b = torch.cat((data_p,b),1)\n",
    "Xw_input = torch.cat((X_p_b,w.t()),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dldg2 = torch.autograd.grad(loss,g2,create_graph=True)\n",
    "        #print(dldg2[0][0])\n",
    "        \n",
    "        #dldg2 = torch.stack(list(dldg2),dim=0)\n",
    "        #dldg2 = dldg2.squeeze(0)\n",
    "        \n",
    "        # first approach is to calculate the update in closed form \n",
    "        l = []\n",
    "        for param in model.parameters():\n",
    "            l.append(param)\n",
    "        # extract w and b\n",
    "        w = torch.cat((l[0],l[1].unsqueeze(0)))\n",
    "        \n",
    "        \n",
    "        update = torch.matmul(data_p,l[0]*dldg2[0][0])\n",
    "        identity = torch.ones(len(data_p),1)\n",
    "        identity = identity.to('cuda')\n",
    "        #update1 = - torch.matmul((output_p - target_p)*identity,dldg2)\n",
    "        update1 = -(output_p - target_p)*identity * dldg2[0][0]\n",
    "\n",
    "        #print(\"approach 1 update:{}\".format(update1))\n",
    "        \n",
    "        # Still bugs in Approach 2 and 3, needs to be fixed\n",
    "        # second approach is to use torch.autograd.functional.hessian to calculate the update\n",
    "        Xw_input = torch.cat((data_p,w[0].unsqueeze(0).t()),0)\n",
    "        def function_f(x):\n",
    "            y_hat = torch.matmul(x[:(len(x)-1)],x[(len(x)-1):].t())+w[1]\n",
    "            return(0.5 * criterion(y_hat,target_p))\n",
    "        f_x = function_f(Xw_input)\n",
    "        hessian = torch.autograd.functional.hessian(function_f, Xw_input)\n",
    "        hessian_wx = hessian[len(Xw_input)-1]\n",
    "        hessian_wx = hessian_wx.squeeze(0)        \n",
    "            \n",
    "        #update2 = torch.matmul(hessian_wx,dldg2[0]).unsqueeze(1)\n",
    "        #print(\"approach2 update:{}\".format(update2))\n",
    "        \n",
    "        # third approach is to use torch.autograd.functional.hvp to calculate the update\n",
    "        v = torch.ones(len(data_p)+1,1) \n",
    "        v = v.to('cuda')\n",
    "        #v = v * dldg2[0]\n",
    "        v[len(v)-1] = 0\n",
    "        update3 = torch.autograd.functional.hvp(function_f,Xw_input,v)\n",
    "        #print(update3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random tests, may be useful\n",
    "criterion = nn.MSELoss()\n",
    "a = torch.ones(3)\n",
    "inputs = torch.ones(10, 1)\n",
    "def function_f(x):\n",
    "    y_hat = torch.matmul(x[:(len(x)-1)],x[(len(x)-1):].t())\n",
    "    return(criterion(y_hat,a))\n",
    "function_f(inputs)\n",
    "print(torch.autograd.functional.hessian(function_f, inputs)[2].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8822, 1.5141],\n",
      "        [0.5289, 4.5655]])\n",
      "tensor([[2.6943, 5.3088]])\n",
      "tensor(0.4605)\n",
      "tensor([[[[0.8822, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 1.5141],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000],\n",
      "          [0.5289, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 4.5655]]]])\n",
      "tensor([[[[2.6943, 0.0000]],\n",
      "\n",
      "         [[0.0000, 5.3088]]]])\n"
     ]
    }
   ],
   "source": [
    "def pow_reducer(x):\n",
    "    return x.pow(3).sum()\n",
    "    \n",
    "inputs1 = torch.rand(2,2)\n",
    "inputs2 = torch.rand(1,2)\n",
    "print(6*inputs1)\n",
    "print(6*inputs2)\n",
    "#inputs2 = inputs1[0]\n",
    "#a = torch.ones(1,2)\n",
    "#b = torch.zeros(2,2)\n",
    "#v = torch.cat((b,a.t()),0)\n",
    "print(pow_reducer(inputs1))\n",
    "print(torch.autograd.functional.hessian(pow_reducer, inputs1))\n",
    "print(torch.autograd.functional.hessian(pow_reducer, inputs2))\n",
    "#torch.autograd.functional.hvp(pow_reducer, inputs,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for param in model.parameters():\n",
    "    l.append(param)\n",
    "\n",
    "# extract w and b\n",
    "w = torch.cat((l[0],l[1].unsqueeze(0)),0)\n",
    "print(w.t().size())\n",
    "\n",
    "b = torch.ones(num_examples_train)\n",
    "b = b.unsqueeze(1)\n",
    "b = b.to('cuda')\n",
    "\n",
    "# absorb b into weights\n",
    "X_train_b = torch.cat((X_train,b),1)\n",
    "print(X_train_b.size())\n",
    "\n",
    "# retrieve prediction using matrix multiplication\n",
    "y_hat = torch.matmul(X_train_b,w)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([10000, 2])\n",
      "tensor([[3.6727],\n",
      "        [4.4504],\n",
      "        [0.9949],\n",
      "        ...,\n",
      "        [5.0574],\n",
      "        [5.3945],\n",
      "        [5.1862]], device='cuda:0', grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for param in model.parameters():\n",
    "    l.append(param)\n",
    "\n",
    "# extract w and b\n",
    "w = torch.cat((l[0],l[1].unsqueeze(0)),0)\n",
    "print(w.t().size())\n",
    "\n",
    "b = torch.ones(num_examples_train)\n",
    "b = b.unsqueeze(1)\n",
    "b = b.to('cuda')\n",
    "\n",
    "# absorb b into weights\n",
    "X_train_b = torch.cat((X_train,b),1)\n",
    "print(X_train_b.size())\n",
    "\n",
    "# retrieve prediction using matrix multiplication\n",
    "y_hat = torch.matmul(X_train_b,w)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
