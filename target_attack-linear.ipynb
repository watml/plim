{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target parameter attack on linear regression with close form solution on the cross derivative\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from models import *\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 1\n",
    "num_outputs = 1\n",
    "num_examples_train = 10000\n",
    "num_examples_test = 5000\n",
    "dtype = torch.float\n",
    "\n",
    "def real_fn(X):\n",
    "    return 2 * X + 4.2\n",
    "    #return 2 * X[:, 0] - 3.4 * X[:, 1] + 4.2\n",
    "\n",
    "# define training set\n",
    "X_train = torch.randn(num_examples_train, num_inputs, device=device, dtype=dtype)\n",
    "noise_train = .1 * torch.randn(num_examples_train, num_inputs, device=device, dtype=dtype)\n",
    "y_train = (real_fn(X_train) + noise_train)\n",
    "\n",
    "X_test = torch.randn(num_examples_test, num_inputs, device=device, dtype=dtype)\n",
    "noise_test = .1 * torch.randn(num_examples_test, num_inputs, device=device, dtype=dtype)\n",
    "y_test = (real_fn(X_test) + noise_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        assert X.size()[0] == y.size()[0]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.size()[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "train_data = DataLoader(LinearDataset(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "test_data = DataLoader(LinearDataset(X_test, y_test), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LinearRegression, self).__init__(**kwargs)\n",
    "        self.fc1 = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "model = LinearRegression()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"models/linear_gd.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the script for training target attack\n",
    "epsilon = 1\n",
    "\n",
    "def autograd(outputs, inputs, create_graph=False):\n",
    "    \"\"\"Compute gradient of outputs w.r.t. inputs, assuming outputs is a scalar.\"\"\"\n",
    "    #inputs = tuple(inputs)\n",
    "    grads = torch.autograd.grad(outputs, inputs, create_graph=create_graph, allow_unused=True)\n",
    "    return [xx if xx is not None else yy.new_zeros(yy.size()) for xx, yy in zip(grads, inputs)]\n",
    "\n",
    "def train(epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_data):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data.requires_grad=True\n",
    "        if epoch==0:\n",
    "            # initialize poisoned data\n",
    "            data_p = Variable(data[:(int(epsilon*len(data)))])\n",
    "            target_p = Variable(target[:(int(epsilon*len(target)))])\n",
    "            max_value = torch.max(data_p)\n",
    "            min_value = torch.min(data_p)\n",
    "            torch.save(target_p,'target_p_linear.pt')\n",
    "        else:\n",
    "            data_p = torch.load('data_p_linear.pt')\n",
    "            target_p = torch.load('target_p_linear.pt')\n",
    "            max_value = torch.max(data_p)\n",
    "            min_value = torch.min(data_p)\n",
    "        data_p.requires_grad=True\n",
    "    \n",
    "        # initialize f function\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # calculate gradient of w on clean sample\n",
    "        output_c = model(data.view(data.size(0), -1))\n",
    "        loss_c =  0.5 * criterion(output_c,target)\n",
    "        \n",
    "        # calculate dL/dg_1\n",
    "        grad_c= autograd(loss_c,tuple(model.parameters()),create_graph=True)\n",
    "        g1 = torch.cat((grad_c[0],grad_c[1].unsqueeze(0)),0)\n",
    "        \n",
    "        # calculate gradient of w on poisoned sample\n",
    "        output_p = model(data_p.view(data_p.size(0), -1))\n",
    "        loss_p = 0.5 * criterion(output_p,target_p)\n",
    "        grad_p= autograd(loss_p,tuple(model.parameters()),create_graph=True)\n",
    "        g2 = torch.cat((grad_p[0],grad_p[1].unsqueeze(0)),0)\n",
    "        \n",
    "        # calculate the true loss: |g_c + g_p|_{inf}\n",
    "        grad_sum = g1+g2\n",
    "        \n",
    "        loss = torch.norm(grad_sum,2)\n",
    "        \n",
    "        dldg2 = torch.autograd.grad(loss,g2,create_graph=True)\n",
    "        dldg2 = torch.stack(list(dldg2),dim=0)\n",
    "        dldg2 = dldg2.squeeze(0)\n",
    "        \n",
    "        # first approach is to calculate the update in closed form \n",
    "        l = []\n",
    "        for param in model.parameters():\n",
    "            l.append(param)\n",
    "        # extract w and b\n",
    "        w = torch.cat((l[0],l[1].unsqueeze(0)))\n",
    "        \n",
    "        update = torch.matmul(data_p,torch.matmul(w.t(),dldg2))\n",
    "        identity = torch.ones(len(data_p),2)\n",
    "        identity = identity.to('cuda')\n",
    "        update1 = - torch.matmul((output_p - target_p)*identity,dldg2)\n",
    "        #print(\"approach 1 update:{}\".format(update1))\n",
    "        \n",
    "        # Still bugs in Approach 2 and 3, needs to be fixed\n",
    "        # second approach is to use torch.autograd.functional.hessian to calculate the update\n",
    "        Xw_input = torch.cat((data_p,w[0].unsqueeze(0).t()),0)\n",
    "        def function_f(x):\n",
    "            y_hat = torch.matmul(x[:(len(x)-1)],x[(len(x)-1):].t())+w[1]\n",
    "            return(0.5 * criterion(y_hat,target_p))\n",
    "        f_x = function_f(Xw_input)\n",
    "        hessian = torch.autograd.functional.hessian(function_f, Xw_input)\n",
    "        hessian_wx = hessian[len(Xw_input)-1]\n",
    "        hessian_wx = hessian_wx.squeeze(0)        \n",
    "            \n",
    "        update2 = torch.matmul(hessian_wx,dldg2[0]).unsqueeze(1)\n",
    "        print(\"approach2 update:{}\".format(update2))\n",
    "        \n",
    "        # third approach is to use torch.autograd.functional.hvp to calculate the update\n",
    "        v = torch.ones(len(data_p)+1,1) \n",
    "        v = v.to('cuda')\n",
    "        v = v * dldg2[0]\n",
    "        v[len(v)-1] = 0\n",
    "        update3 = torch.autograd.functional.hvp(function_f,Xw_input,v)\n",
    "        print(update3[1])\n",
    "        \n",
    "        # to use approach 1:\n",
    "        #data_t = data_p + update1\n",
    "        \n",
    "        # to use approach 2:\n",
    "        data_t = data_p + update2[:len(update2)-1]\n",
    "        #data_t = torch.clamp(data_t,min_value.item(),max_value.item())\n",
    "        #print(\"data change:{}\".format(torch.mean(data_t-data_p)))\n",
    "        torch.save(data_t, 'data_p_linear.pt')\n",
    "        \n",
    "        print(\"epoch:{},loss:{}\".format(epoch, loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approach2 update:tensor([[ 6.0077e-06],\n",
      "        [-2.1764e-05],\n",
      "        [ 9.3778e-05],\n",
      "        ...,\n",
      "        [-4.9321e-05],\n",
      "        [-4.4570e-05],\n",
      "        [-4.4095e-01]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([[-1.0692e-04],\n",
      "        [-1.0692e-04],\n",
      "        [-1.0692e-04],\n",
      "        ...,\n",
      "        [-1.0692e-04],\n",
      "        [-1.0692e-04],\n",
      "        [-3.9847e-01]], device='cuda:0')\n",
      "epoch:0,loss:2.041837692260742\n",
      "approach2 update:tensor([[ 6.0075e-06],\n",
      "        [-2.1763e-05],\n",
      "        [ 9.3775e-05],\n",
      "        ...,\n",
      "        [-4.9320e-05],\n",
      "        [-4.4569e-05],\n",
      "        [-4.4096e-01]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([[-1.0693e-04],\n",
      "        [-1.0693e-04],\n",
      "        [-1.0693e-04],\n",
      "        ...,\n",
      "        [-1.0693e-04],\n",
      "        [-1.0693e-04],\n",
      "        [-3.9846e-01]], device='cuda:0')\n",
      "epoch:1,loss:2.0418219566345215\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-da3da09222d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-126-adb8820ebc11>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mf_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXw_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mhessian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXw_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mhessian_wx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhessian\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXw_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mhessian_wx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhessian_wx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjac_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tuple_postprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_inputs_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inputs_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             vj = _autograd_grad((out.reshape(-1)[j],), inputs,\n\u001b[0;32m--> 438\u001b[0;31m                                 retain_graph=True, create_graph=create_graph)\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mel_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjac_i_el\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvj_el\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_el\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjac_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         return torch.autograd.grad(new_outputs, inputs, new_grad_outputs, allow_unused=True,\n\u001b[0;32m--> 147\u001b[0;31m                                    create_graph=create_graph, retain_graph=retain_graph)\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fill_in_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    202\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    203\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.ones(int(num_examples_train*epsilon))\n",
    "b = b.unsqueeze(1)\n",
    "b = b.to('cuda')\n",
    "# absorb b into the input so it matches the weights in tensors\n",
    "X_p_b = torch.cat((data_p,b),1)\n",
    "Xw_input = torch.cat((X_p_b,w.t()),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random tests, may be useful\n",
    "criterion = nn.MSELoss()\n",
    "a = torch.ones(3)\n",
    "inputs = torch.ones(10, 1)\n",
    "def function_f(x):\n",
    "    y_hat = torch.matmul(x[:(len(x)-1)],x[(len(x)-1):].t())\n",
    "    return(criterion(y_hat,a))\n",
    "function_f(inputs)\n",
    "print(torch.autograd.functional.hessian(function_f, inputs)[2].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8822, 1.5141],\n",
      "        [0.5289, 4.5655]])\n",
      "tensor([[2.6943, 5.3088]])\n",
      "tensor(0.4605)\n",
      "tensor([[[[0.8822, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 1.5141],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000],\n",
      "          [0.5289, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 4.5655]]]])\n",
      "tensor([[[[2.6943, 0.0000]],\n",
      "\n",
      "         [[0.0000, 5.3088]]]])\n"
     ]
    }
   ],
   "source": [
    "def pow_reducer(x):\n",
    "    return x.pow(3).sum()\n",
    "    \n",
    "inputs1 = torch.rand(2,2)\n",
    "inputs2 = torch.rand(1,2)\n",
    "print(6*inputs1)\n",
    "print(6*inputs2)\n",
    "#inputs2 = inputs1[0]\n",
    "#a = torch.ones(1,2)\n",
    "#b = torch.zeros(2,2)\n",
    "#v = torch.cat((b,a.t()),0)\n",
    "print(pow_reducer(inputs1))\n",
    "print(torch.autograd.functional.hessian(pow_reducer, inputs1))\n",
    "print(torch.autograd.functional.hessian(pow_reducer, inputs2))\n",
    "#torch.autograd.functional.hvp(pow_reducer, inputs,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for param in model.parameters():\n",
    "    l.append(param)\n",
    "\n",
    "# extract w and b\n",
    "w = torch.cat((l[0],l[1].unsqueeze(0)),0)\n",
    "print(w.t().size())\n",
    "\n",
    "b = torch.ones(num_examples_train)\n",
    "b = b.unsqueeze(1)\n",
    "b = b.to('cuda')\n",
    "\n",
    "# absorb b into weights\n",
    "X_train_b = torch.cat((X_train,b),1)\n",
    "print(X_train_b.size())\n",
    "\n",
    "# retrieve prediction using matrix multiplication\n",
    "y_hat = torch.matmul(X_train_b,w)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([10000, 2])\n",
      "tensor([[3.6727],\n",
      "        [4.4504],\n",
      "        [0.9949],\n",
      "        ...,\n",
      "        [5.0574],\n",
      "        [5.3945],\n",
      "        [5.1862]], device='cuda:0', grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for param in model.parameters():\n",
    "    l.append(param)\n",
    "\n",
    "# extract w and b\n",
    "w = torch.cat((l[0],l[1].unsqueeze(0)),0)\n",
    "print(w.t().size())\n",
    "\n",
    "b = torch.ones(num_examples_train)\n",
    "b = b.unsqueeze(1)\n",
    "b = b.to('cuda')\n",
    "\n",
    "# absorb b into weights\n",
    "X_train_b = torch.cat((X_train,b),1)\n",
    "print(X_train_b.size())\n",
    "\n",
    "# retrieve prediction using matrix multiplication\n",
    "y_hat = torch.matmul(X_train_b,w)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
